{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f6c5e7",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b5983",
   "metadata": {},
   "source": [
    "###### The aim of this project is to propose a label of various fruit images. Fruit dataset has been downloaded from the Kaggle. Build the CNN model with Convolutional, pooling and Flatten layer. Adding batch normalization and dropout layer makes simpler model to increase the accuracy and decrease the loss at the same time. Batch Normalization improves the performance and stability during training. Dropout minimize the overfitting of the model. Using those layers and hyper-parameters, Test the classifier and graph the accuracy and loss with 6 optimizers which are adam, sgd, adamax, adagrad, Adadelta, Nadam, and Ftrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad233fb",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765bcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136b94c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac96ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_file= \"fruits-360/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f30f323c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of fruits to model\n",
    "#fruit_list = [\"Kiwi\", \"Banana\", \"Avocado ripe\",\"Apricot\", \"Avocado\",\"Blueberry\",\"Corn\",\"Cocos\", \"Clementine\", \"Dates\",\"Mandarine\", \"Orange\",\n",
    "              #  \"Limes\", \"Fig\",\"Lemon\", \"Pear\",\"Peach\", \"Plum\", \"Raspberry\", \"Strawberry\", \"Pineapple\", \"Pomegranate\",\"Raspberry\"]\n",
    "fruit_list = [\"Kiwi\", \"Banana\",\"Apricot\", \"Avocado\",\"Cocos\", \"Clementine\", \"Dates\",\"Mandarine\", \"Orange\",\n",
    "                \"Limes\",\"Lemon\",\"Peach\", \"Plum\", \"Strawberry\", \"Pineapple\", \"Pomegranate\"]\n",
    "# number of fruit classes (i.e. fruits)\n",
    "number_of_class = len(fruit_list)\n",
    "number_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcf2f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train : Validation = 8:2\n",
    "train_data_generator = ImageDataGenerator(\n",
    "  rescale = 1/255,\n",
    "  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c5aacb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6171 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_image_array = train_data_generator.flow_from_directory(train_image_file, \n",
    "                                                \n",
    "                                                    subset = 'training',\n",
    "                                                    target_size = (32,32),       \n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    classes = fruit_list,\n",
    "                                                    batch_size = 32,\n",
    "                                                    seed = 123)\n",
    "#Found 6171 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "825be0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1538 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# validation images\n",
    "valid_image_array = train_data_generator.flow_from_directory(train_image_file, \n",
    "                                                    subset = 'validation',\n",
    "                                                    target_size = (32,32),\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    classes = fruit_list,\n",
    "                                                    batch_size = 32,\n",
    "                                                    seed = 123)\n",
    "#Found 1538 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dae4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training samples\n",
    "train_samples = train_image_array.samples\n",
    "train_samples\n",
    "# number of validation samples\n",
    "valid_samples = valid_image_array.samples\n",
    "\n",
    "# define number of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57bb42f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184822f2",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba48ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#padding = \"same\" --> apply padding --> restrict reducing the image size. \n",
    "model.add(Conv2D(16, (3, 3), input_shape = (32, 32, 3),  padding = \"same\", activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "# Use max pooling\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "   #Bath norm vs Dropout\n",
    "  #Batch increase to speed up optimization\n",
    "  #dropout -> to control the overfitting \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_class))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91447af",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97069fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "192/192 [==============================] - 29s 153ms/step - loss: 0.6333 - accuracy: 0.8197 - val_loss: 4.3788 - val_accuracy: 0.2370\n",
      "Epoch 2/10\n",
      "192/192 [==============================] - 24s 123ms/step - loss: 0.1261 - accuracy: 0.9580 - val_loss: 1.3400 - val_accuracy: 0.6230\n",
      "Epoch 3/10\n",
      "192/192 [==============================] - 24s 124ms/step - loss: 0.0828 - accuracy: 0.9700 - val_loss: 0.0345 - val_accuracy: 0.9824\n",
      "Epoch 4/10\n",
      "192/192 [==============================] - 23s 122ms/step - loss: 0.0767 - accuracy: 0.9749 - val_loss: 0.0029 - val_accuracy: 0.9987\n",
      "Epoch 5/10\n",
      "192/192 [==============================] - 22s 115ms/step - loss: 0.0553 - accuracy: 0.9811 - val_loss: 0.1430 - val_accuracy: 0.9518\n",
      "Epoch 6/10\n",
      "192/192 [==============================] - 22s 116ms/step - loss: 0.0582 - accuracy: 0.9831 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "192/192 [==============================] - 23s 117ms/step - loss: 0.0424 - accuracy: 0.9852 - val_loss: 1.6788e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "192/192 [==============================] - 22s 116ms/step - loss: 0.0527 - accuracy: 0.9853 - val_loss: 0.1818 - val_accuracy: 0.9460\n",
      "Epoch 9/10\n",
      "192/192 [==============================] - 22s 117ms/step - loss: 0.0764 - accuracy: 0.9800 - val_loss: 0.3475 - val_accuracy: 0.9329\n",
      "Epoch 10/10\n",
      "192/192 [==============================] - 24s 123ms/step - loss: 0.0584 - accuracy: 0.9876 - val_loss: 0.1280 - val_accuracy: 0.9831\n",
      "Found 7709 images belonging to 16 classes.\n",
      "7709/7709 [==============================] - 41s 5ms/step - loss: 0.0307 - accuracy: 0.9952\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "adam_accuracy = history.history['accuracy']\n",
    "adam_val_accuracy = history.history['val_accuracy']\n",
    "adam_loss = history.history['loss']\n",
    "adam_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "adam_Evaluation_Loss=evaluation[0]\n",
    "adam_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(adam_accuracy))\n",
    "plt.plot(epochs, adam_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, adam_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, adam_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, adam_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876739c0",
   "metadata": {},
   "source": [
    "### SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "sgd_accuracy = history.history['accuracy']\n",
    "sgd_val_accuracy = history.history['val_accuracy']\n",
    "sgd_loss = history.history['loss']\n",
    "sgd_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-CNN(R)/fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "sgd_Evaluation_Loss=evaluation[0]\n",
    "sgd_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(sgd_accuracy))\n",
    "plt.plot(epochs, sgd_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, sgd_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, sgd_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, sgd_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc039b",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'RMSprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "RMSprop_accuracy = history.history['accuracy']\n",
    "RMSprop_val_accuracy = history.history['val_accuracy']\n",
    "RMSprop_loss = history.history['loss']\n",
    "RMSprop_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-CNN(R)/fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "RMSprop_Evaluation_Loss=evaluation[0]\n",
    "RMSprop_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(RMSprop_accuracy))\n",
    "plt.plot(epochs, RMSprop_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, RMSprop_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, RMSprop_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, RMSprop_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e04e72",
   "metadata": {},
   "source": [
    "### Adadelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109138aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adadelta', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "Adadelta_accuracy = history.history['accuracy']\n",
    "Adadelta_val_accuracy = history.history['val_accuracy']\n",
    "Adadelta_loss = history.history['loss']\n",
    "Adadelta_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-CNN(R)/fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "Adadelta_Evaluation_Loss=evaluation[0]\n",
    "Adadelta_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6317afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(RMSprop_accuracy))\n",
    "plt.plot(epochs, Adadelta_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, Adadelta_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, Adadelta_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, Adadelta_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33dabd",
   "metadata": {},
   "source": [
    "### Adagrad Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47da2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adagrad', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "Adagrad_accuracy = history.history['accuracy']\n",
    "Adagrad_val_accuracy = history.history['val_accuracy']\n",
    "Adagrad_loss = history.history['loss']\n",
    "Adagrad_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-CNN(R)/fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "Adagrad_Evaluation_Loss=evaluation[0]\n",
    "Adagrad_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5eb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(RMSprop_accuracy))\n",
    "plt.plot(epochs, Adagrad_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, Adagrad_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, Adagrad_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, Adagrad_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef61b2",
   "metadata": {},
   "source": [
    "### Adamax Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adamax', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "Adamax_accuracy = history.history['accuracy']\n",
    "Adamax_val_accuracy = history.history['val_accuracy']\n",
    "Adamax_loss = history.history['loss']\n",
    "Adamax_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-CNN(R)/fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "Adamax_Evaluation_Loss=evaluation[0]\n",
    "Adamax_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06916067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(RMSprop_accuracy))\n",
    "plt.plot(epochs, Adamax_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, Adamax_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, Adamax_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, Adamax_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2741df",
   "metadata": {},
   "source": [
    "### Nadam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Nadam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "Nadam_accuracy = history.history['accuracy']\n",
    "Nadam_val_accuracy = history.history['val_accuracy']\n",
    "Nadam_loss = history.history['loss']\n",
    "Nadam_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-CNN(R)/fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "Nadam_Evaluation_Loss=evaluation[0]\n",
    "Nadam_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f235224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(RMSprop_accuracy))\n",
    "plt.plot(epochs, Nadam_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, Nadam_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, Nadam_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, Nadam_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce50097",
   "metadata": {},
   "source": [
    "### Ftrl Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Ftrl', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit(\n",
    "  # training data\n",
    "  train_image_array,\n",
    "  steps_per_epoch = int(train_samples / 32), \n",
    "  epochs = 10,\n",
    "  # validation data\n",
    "  validation_data = valid_image_array,\n",
    "  validation_steps = int(valid_samples / 32)\n",
    ")\n",
    "Ftrl_accuracy = history.history['accuracy']\n",
    "Ftrl_val_accuracy = history.history['val_accuracy']\n",
    "Ftrl_loss = history.history['loss']\n",
    "Ftrl_val_loss = history.history['val_loss']\n",
    "test_image_file = \"fruits-CNN(R)/fruits-360/Training\"\n",
    "test_data_generator = ImageDataGenerator(rescale = 1/255)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "  test_image_file,\n",
    "  target_size = (32,32),\n",
    "  class_mode = \"categorical\",\n",
    "  classes = fruit_list,\n",
    "  batch_size = 1,\n",
    "  shuffle = False,\n",
    "  seed = 123)\n",
    "#Found 2592 images\n",
    "evaluation = model.evaluate(test_generator, \n",
    "                     steps = int(test_generator.samples))\n",
    "FTrl_Evaluation_Loss=evaluation[0]\n",
    "Ftrl_Evaluation_accuracy = evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "epochs = range(len(RMSprop_accuracy))\n",
    "plt.plot(epochs, Ftrl_accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, Ftrl_val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, Ftrl_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, Ftrl_val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b0596",
   "metadata": {},
   "source": [
    "### Combined Accuracy & Loss Graph of all optimizer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 30})\n",
    "epochs = range(len(adam_accuracy))\n",
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(epochs, adam_val_accuracy, 'g', label='adam')\n",
    "plt.plot(epochs, sgd_val_accuracy, 'r', label='sgd')\n",
    "plt.plot(epochs, RMSprop_val_accuracy, 'c', label='RMSprop')\n",
    "plt.plot(epochs, Adadelta_val_accuracy, 'm', label='Adadelta')\n",
    "plt.plot(epochs, Adagrad_val_accuracy, 'y', label='Adagrad')\n",
    "plt.plot(epochs, Adamax_val_accuracy, 'k', label='Adamax')\n",
    "#plt.plot(epochs, Nadam_val_accuracy, 'w', label='Nadam')\n",
    "#plt.plot(epochs, Ftrl_val_accuracy, 'b', label='Ftrl')\n",
    "plt.title('validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(epochs, adam_val_loss, 'g', label='adam')\n",
    "plt.plot(epochs, sgd_val_loss, 'r', label='sgd')\n",
    "plt.plot(epochs, RMSprop_val_loss, 'c', label='RMSprop')\n",
    "plt.plot(epochs, Adadelta_val_loss, 'm', label='Adadelta')\n",
    "plt.plot(epochs, Adagrad_val_loss, 'y', label='Adagrad')\n",
    "plt.plot(epochs, Adamax_val_loss, 'k', label='Adamax')\n",
    "#plt.plot(epochs, Nadam_val_loss, 'w', label='Nadam')\n",
    "#plt.plot(epochs, Ftrl_val_loss, 'b', label='Ftrl')\n",
    "plt.title('validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33499449",
   "metadata": {},
   "source": [
    "### Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fffc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "G=[['adam','sgd','RMSprop','Adadelta','Adagrad','Adamax','Ftrl'],\n",
    " [adam_Evaluation_Loss,sgd_Evaluation_Loss,RMSprop_Evaluation_Loss,Adadelta_Evaluation_Loss,Adagrad_Evaluation_Loss,Adamax_Evaluation_Loss,FTrl_Evaluation_Loss],\n",
    "[adam_Evaluation_accuracy,sgd_Evaluation_accuracy,RMSprop_Evaluation_accuracy,Adadelta_Evaluation_accuracy,Adagrad_Evaluation_accuracy,Adamax_Evaluation_accuracy,Ftrl_Evaluation_accuracy]]\n",
    "X_lab=G[0][:]\n",
    "Acc=G[2][:]\n",
    "Loss = G[1][:]\n",
    "Acc=([round(x,4) for x in Acc])\n",
    "Loss=([round(x,4) for x in Loss])\n",
    "\n",
    "width = 0.3\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=2)\n",
    "fig.set_size_inches(20,20)\n",
    "accGraph=ax1.bar(X_lab, Acc, width = 0.3, color = 'y')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xlabel('Optimizer')\n",
    "ax1.set_title('Test Accuracy')\n",
    "\n",
    "lossGraph=ax2.bar(X_lab, Loss, width = 0.3, color = 'r')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Optimizer')\n",
    "ax2.set_title('Test Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183005d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb02b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9206e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
